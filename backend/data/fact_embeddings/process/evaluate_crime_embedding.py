import json
import numpy as np
import pickle
from tqdm import tqdm
import random
import os

def evaluate_crime_centroids(
    embedding_path: str,
    data_path: str,
    centroid_path: str,
    sample_size: int = 20000,
    top_k_list=(1, 3, 5, 10),
    seed: int = 42
):
    """
    ‰ΩøÁî® crime_centroids.pkl ÂØπÈöèÊú∫Ê†∑Êú¨ËøõË°åÁΩ™ÂêçÈ¢ÑÊµãÔºåÂπ∂ËÆ°ÁÆó Recall@K„ÄÇ
    
    Args:
        embedding_path: fact_embeddings.npy Ë∑ØÂæÑ
        data_path: train.json (JSONL)
        centroid_path: crime_centroids.pkl Ë∑ØÂæÑ
        sample_size: ÈöèÊú∫ÊäΩÂèñÊ†∑Êú¨Êï∞
        top_k_list: Ë¶ÅËØÑ‰º∞ÁöÑ K ÂÄº
        seed: ÈöèÊú∫ÁßçÂ≠ê
    """
    print("üîç Loading embeddings...")
    embeddings = np.load(embedding_path)  # (N, D)
    N, D = embeddings.shape
    print(f"‚úÖ Embeddings: {N:,} samples, {D} dims")

    print("üîç Loading crime centroids...")
    with open(centroid_path, 'rb') as f:
        crime_stats = pickle.load(f)
    
    # ÊûÑÂª∫ÁΩ™ÂêçÂàóË°®Âíå centroid Áü©Èòµ
    crimes = sorted(crime_stats.keys())  # Âõ∫ÂÆöÈ°∫Â∫è
    num_crimes = len(crimes)
    centroids = np.stack([crime_stats[crime]["centroid"] for crime in crimes])  # (C, D)
    print(f"‚úÖ Loaded {num_crimes} crime centroids.")

    # È™åËØÅÔºöÊòØÂê¶ÂΩí‰∏ÄÂåñÔºüÔºàBGE-M3 ÈªòËÆ§ÊòØÔºâ
    norm_diff = np.abs(np.linalg.norm(centroids, axis=1) - 1.0)
    if norm_diff.max() > 1e-3:
        print("‚ö†Ô∏è Warning: Centroids not normalized! Normalizing now.")
        centroids = centroids / np.linalg.norm(centroids, axis=1, keepdims=True)

    # Ëé∑ÂèñÊâÄÊúâÁúüÂÆûÁΩ™ÂêçÔºàÁî®‰∫éÊäΩÊ†∑Ôºâ
    print("üîç Reading all accusations for sampling...")
    all_accusations = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                acc = item.get("meta", {}).get("accusation", [])
                all_accusations.append(acc)
            except:
                all_accusations.append([])

    if len(all_accusations) != N:
        print(f"‚ùó Data length mismatch! Using min({len(all_accusations)}, {N})")
        N = min(len(all_accusations), len(embeddings))
        embeddings = embeddings[:N]
        all_accusations = all_accusations[:N]

    # ÈöèÊú∫ÊäΩÊ†∑
    random.seed(seed)
    np.random.seed(seed)
    indices = np.random.choice(N, size=min(sample_size, N), replace=False)
    print(f"üé≤ Selected {len(indices)} random samples for evaluation.")

    # ÂàùÂßãÂåñËÆ°Êï∞Âô®
    hit_counts = {k: 0 for k in top_k_list}
    total_valid = 0  # ÊéíÈô§Êó†ÁΩ™ÂêçÊ†∑Êú¨

    print("üîÑ Running evaluation...")
    for idx in tqdm(indices, desc="Evaluating"):
        true_crimes = all_accusations[idx]
        if not true_crimes:
            continue
        total_valid += 1

        query_emb = embeddings[idx].reshape(1, -1)  # (1, D)

        # ËÆ°ÁÆó‰∏éÊâÄÊúâ centroid ÁöÑÁõ∏‰ººÂ∫¶Ôºàcosine = dot product if normalizedÔºâ
        similarities = np.dot(query_emb, centroids.T).flatten()  # (C,)

        # Ëé∑Âèñ top-K È¢ÑÊµãÁΩ™Âêç
        top_k_indices = np.argsort(-similarities)  # descending
        predicted_crimes = [crimes[i] for i in top_k_indices]

        # Ê£ÄÊü• recall@k
        true_set = set(true_crimes)
        for k in top_k_list:
            top_k_pred = set(predicted_crimes[:k])
            if true_set & top_k_pred:  # Êúâ‰∫§ÈõÜÂç≥ÂëΩ‰∏≠
                hit_counts[k] += 1

    # ËÆ°ÁÆó recall
    print("\nüìä Evaluation Results:")
    print(f"Total valid samples: {total_valid:,}")
    recalls = {}
    for k in top_k_list:
        recall = hit_counts[k] / total_valid if total_valid > 0 else 0.0
        recalls[k] = recall
        print(f"  Recall@{k:2d}: {recall:.4f} ({hit_counts[k]}/{total_valid})")

    # ‰øùÂ≠òÁªìÊûú
    result = {
        "sample_size": len(indices),
        "valid_samples": total_valid,
        "recall": {f"R@{k}": round(recalls[k], 4) for k in top_k_list}
    }
    result_path = os.path.splitext(centroid_path)[0] + "_eval.json"
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ Results saved to {result_path}")

    return recalls


if __name__ == "__main__":
    EMBEDDING_PATH = r"F:\LegalAgent\backend\data\fact_embeddings\fact_embeddings_bge-m3.npy"
    DATA_PATH = r"F:\LegalAgent\dataset\final_all_data\first_stage\train.json"
    CENTROID_PATH = r"F:\LegalAgent\backend\data\fact_embeddings\crime_centroids.pkl"

    evaluate_crime_centroids(
        embedding_path=EMBEDDING_PATH,
        data_path=DATA_PATH,
        centroid_path=CENTROID_PATH,
        sample_size=200000,
        top_k_list=(1, 3, 5, 10),
        seed=42
    )