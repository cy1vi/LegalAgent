import json
import requests
import time
from tqdm import tqdm
from typing import List

# é…ç½®
API_URL = "http://localhost:4240/batch_search"
DATASET_PATH = r"F:\LegalAgent\dataset\final_all_data\first_stage\train.json"
OUTPUT_PATH = r"F:\LegalAgent\backend\sparse-embedding\data\eval_results.jsonl"
BATCH_SIZE = 50  # æ¯æ‰¹å‘é€å¤šå°‘æ¡æ•°æ® (å»ºè®® 50-100)
TEST_LIMIT = 1000 # æµ‹è¯•å¤šå°‘æ¡æ•°æ® (None è¡¨ç¤ºè·‘å…¨é‡)

def load_dataset(path, limit=None):
    data = []
    print(f"ðŸ“– Loading dataset from {path}...")
    with open(path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if limit and i >= limit:
                break
            try:
                item = json.loads(line)
                # å‡è®¾æ¯è¡Œéƒ½æœ‰ fact å­—æ®µ
                if "fact" in item:
                    data.append(item)
            except:
                pass
    return data

def run_evaluation():
    # 1. åŠ è½½æ•°æ®
    dataset = load_dataset(DATASET_PATH, limit=TEST_LIMIT)
    total_samples = len(dataset)
    print(f"âœ… Loaded {total_samples} samples.")

    # 2. å‡†å¤‡ç»“æžœæ–‡ä»¶
    f_out = open(OUTPUT_PATH, 'w', encoding='utf-8')

    # 3. åˆ†æ‰¹å¤„ç†
    total_time = 0
    success_count = 0
    
    # è¿›åº¦æ¡
    pbar = tqdm(total=total_samples, unit="doc")
    
    for i in range(0, total_samples, BATCH_SIZE):
        batch_items = dataset[i : i + BATCH_SIZE]
        batch_facts = [item['fact'] for item in batch_items]
        
        payload = {
            "facts": batch_facts,
            "top_k": 5
        }
        
        try:
            t0 = time.time()
            resp = requests.post(API_URL, json=payload)
            
            if resp.status_code == 200:
                results_list = resp.json()
                batch_time = time.time() - t0
                total_time += batch_time
                
                # å†™å…¥ç»“æžœ
                for original_item, search_res in zip(batch_items, results_list):
                    output_line = {
                        "query_fact": original_item['fact'],
                        "ground_truth_meta": original_item.get('meta', {}),
                        "retrieved_docs": search_res
                    }
                    f_out.write(json.dumps(output_line, ensure_ascii=False) + "\n")
                    success_count += 1
            else:
                print(f"âŒ Batch failed: {resp.status_code} - {resp.text}")
                
        except Exception as e:
            print(f"âŒ Request error: {e}")
            
        pbar.update(len(batch_items))

    pbar.close()
    f_out.close()

    # 4. ç»Ÿè®¡
    avg_time = (total_time / success_count * 1000) if success_count > 0 else 0
    print("\n" + "="*40)
    print(f"ðŸ“Š Evaluation Complete")
    print(f"   - Total Processed: {success_count}/{total_samples}")
    print(f"   - Total Time: {total_time:.2f}s")
    print(f"   - Avg Latency: {avg_time:.2f}ms / doc")
    print(f"   - Throughput: {success_count / total_time:.2f} docs/s")
    print(f"ðŸ’¾ Results saved to: {OUTPUT_PATH}")
    print("="*40)

if __name__ == "__main__":
    run_evaluation()