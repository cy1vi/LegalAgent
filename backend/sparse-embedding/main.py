import os
import time
import json
import threading
import uvicorn
from concurrent.futures import ThreadPoolExecutor, as_completed 
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import array

from logger import logger
from config import ExtractorConfig, GlobalConfig
from utils import SparseRetriever
from crime_keywords_extractor import CrimeKeywordsExtractor
from universal_fact_extractor import UniversalFactExtractor


class CorpusManager:
    def __init__(self, filepath):
        self.filepath = filepath
        self.line_offsets = array.array('Q')
        self.lock = threading.Lock() 
        try:
            self.file_handle = open(filepath, 'r', encoding='utf-8')
            logger.debug(f"Successfully opened corpus file: {filepath}")
        except Exception as e:
            logger.error(f"Failed to open corpus file: {e}")
            self.file_handle = None
        

    def load_index(self):
        """æ‰«ææ–‡ä»¶ï¼Œå»ºç«‹è¡Œå·åˆ°æ–‡ä»¶åç§»é‡çš„ç´¢å¼•"""
        offset = 0
        with open(self.filepath, 'rb') as f:
            for line in f:
                self.line_offsets.append(offset)
                offset += len(line)

    def get_doc(self, idx: int) -> Dict[str, Any]:
        """æ ¹æ®è¡Œå·è·å–åŸå§‹æ•°æ®"""
        if not self.file_handle or idx >= len(self.line_offsets):
            logger.error(f"Invalid index: {idx}, max: {len(self.line_offsets)}")
            return {}
            
        with self.lock:
            try:
                self.file_handle.seek(self.line_offsets[idx])
                line = self.file_handle.readline()
                doc = json.loads(line)
                return doc
            except Exception as e:
                logger.error(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
                return {}

    def close(self):
        if self.file_handle:
            self.file_handle.close()

# ---------------------------------------------------------
# 1.5. Schema å’Œ Keywords ç®¡ç†å™¨ (ç”¨äºæŒ‰éœ€è¯»å–ç»“æ„åŒ–ä¿¡æ¯å’Œå…³é”®è¯)
# ---------------------------------------------------------
class SchemaKeywordsManager:
    def __init__(self, schema_filepath: str, keywords_filepath: str):
        self.schema_filepath = schema_filepath
        self.keywords_filepath = keywords_filepath
        self.schema_line_offsets = array.array('Q')
        self.keywords_line_offsets = array.array('Q')
        self.schema_file_handle = None
        self.keywords_file_handle = None
        self.lock = threading.Lock() 


    def load_index(self):
        """æ‰«æä¸¤ä¸ªæ–‡ä»¶ï¼Œå»ºç«‹è¡Œå·åˆ°æ–‡ä»¶åç§»é‡çš„ç´¢å¼•"""
        if not os.path.exists(self.schema_filepath):
            logger.error(f"Schemaæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.schema_filepath}")
            return
        if not os.path.exists(self.keywords_filepath):
            logger.error(f"Keywordsæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.keywords_filepath}")
            return

        logger.debug(f"æ­£åœ¨æ„å»ºSchemaç´¢å¼• (æ–‡ä»¶: {self.schema_filepath})...")
        start = time.time()
        try:
            offset = 0
            self.schema_line_offsets = array.array('Q')  # ä¿®æ”¹è¿™é‡Œ
            self.schema_line_offsets.append(offset)
            
            with open(self.schema_filepath, 'rb') as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    offset += len(line)
                    self.schema_line_offsets.append(offset)
            
            self.schema_line_offsets.pop()
            self.schema_file_handle = open(self.schema_filepath, 'r', encoding='utf-8')
            
            elapsed = time.time() - start
            logger.debug(f"Schemaç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self.schema_line_offsets)} æ¡æ•°æ®ï¼Œè€—æ—¶ {elapsed:.2f}s")
        except Exception as e:
            logger.error(f"æ„å»ºSchemaç´¢å¼•å¤±è´¥: {e}")
            raise e

        logger.debug(f"æ­£åœ¨æ„å»ºKeywordsç´¢å¼• (æ–‡ä»¶: {self.keywords_filepath})...")
        start = time.time()
        try:
            offset = 0
            self.keywords_line_offsets = array.array('Q')  # ä¿®æ”¹è¿™é‡Œ
            self.keywords_line_offsets.append(offset)
            
            with open(self.keywords_filepath, 'rb') as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    offset += len(line)
                    self.keywords_line_offsets.append(offset)
            
            self.keywords_line_offsets.pop()
            self.keywords_file_handle = open(self.keywords_filepath, 'r', encoding='utf-8')
            
            elapsed = time.time() - start
            logger.debug(f"Keywordsç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self.keywords_line_offsets)} æ¡æ•°æ®ï¼Œè€—æ—¶ {elapsed:.2f}s")
        except Exception as e:
            logger.error(f"æ„å»ºKeywordsç´¢å¼•å¤±è´¥: {e}")
            raise e

    def get_schema_and_keywords(self, idx: int) -> tuple[dict, dict]:
        """æ ¹æ®è¡Œå·è·å–å¯¹åº”çš„schemaå’Œkeywords"""
        schema_result = {}
        keywords_result = {}
        success = True

        if not self.schema_file_handle or idx >= len(self.schema_line_offsets):
            logger.warning(f"Schemaæ–‡ä»¶å¥æŸ„æ— æ•ˆæˆ–ç´¢å¼•è¶…å‡ºèŒƒå›´: {idx}")
            success = False
        else:
            with self.lock:
                try:
                    offset = self.schema_line_offsets[idx]
                    self.schema_file_handle.seek(offset)
                    line = self.schema_file_handle.readline()
                    item = json.loads(line)
                    schema_result = item.get("universal_fact", {})
                except Exception as e:
                    logger.error(f"è¯»å–Schemaæ•°æ®å¤±è´¥ (Index: {idx}): {e}")
                    success = False

        if not self.keywords_file_handle or idx >= len(self.keywords_line_offsets):
            logger.warning(f"Keywordsæ–‡ä»¶å¥æŸ„æ— æ•ˆæˆ–ç´¢å¼•è¶…å‡ºèŒƒå›´: {idx}")
            success = False
        else:
            with self.lock:
                try:
                    offset = self.keywords_line_offsets[idx]
                    self.keywords_file_handle.seek(offset)
                    line = self.keywords_file_handle.readline()
                    item = json.loads(line)
                    doc_keywords = {}
                    if "sparse_extraction" in item and "keyword_counts" in item["sparse_extraction"]:
                         doc_keywords = item["sparse_extraction"]["keyword_counts"]
                    elif "keyword_counts" in item: 
                        doc_keywords = item["keyword_counts"]
                    keywords_result = doc_keywords
                except Exception as e:
                    logger.error(f"è¯»å–Keywordsæ•°æ®å¤±è´¥ (Index: {idx}): {e}")
                    success = False

        if not success:
            return {}, {}
        return schema_result, keywords_result


    def close(self):
        if self.schema_file_handle:
            self.schema_file_handle.close()
        if self.keywords_file_handle:
            self.keywords_file_handle.close()

# ---------------------------------------------------------
# 2. å…¨å±€å˜é‡ä¸ç”Ÿå‘½å‘¨æœŸ
# ---------------------------------------------------------
retriever: Optional[SparseRetriever] = None
extractor: Optional[CrimeKeywordsExtractor] = None
universal_extractor: Optional[UniversalFactExtractor] = None 
corpus_manager: Optional[CorpusManager] = None
schema_keywords_manager: Optional[SchemaKeywordsManager] = None 

from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æœåŠ¡å¯åŠ¨ä¸å…³é—­çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global retriever, extractor, universal_extractor, corpus_manager,schema_keywords_manager

    logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨ç¨€ç–æ£€ç´¢æœåŠ¡...")

    # 1. åˆå§‹åŒ–å…³é”®è¯æå–å™¨ (æ¢å¤è¿™éƒ¨åˆ†ä»£ç )
    logger.debug(f"åŠ è½½å…³é”®è¯æå–å™¨: {ExtractorConfig.KEYWORDS_FILE}")
    try:
        extractor = CrimeKeywordsExtractor(ExtractorConfig.KEYWORDS_FILE)
    except Exception as e:
        logger.error(f"å…³é”®è¯æå–å™¨åŠ è½½å¤±è´¥: {e}")
        raise e

    # 1.5 åˆå§‹åŒ–é€šç”¨äº‹å®æå–å™¨
    logger.debug(f"åŠ è½½é€šç”¨äº‹å®æå–å™¨ (è§„åˆ™è·¯å¾„: {ExtractorConfig.RULES_YAML_PATH})...")
    try:
        universal_extractor = UniversalFactExtractor(rules_dir=ExtractorConfig.RULES_YAML_PATH)

    except Exception as e:
        logger.error(f"é€šç”¨äº‹å®æå–å™¨åŠ è½½å¤±è´¥: {e}")
        raise e

    # 2. åˆå§‹åŒ–ç¨€ç–æ£€ç´¢å™¨
    try:
        with open(ExtractorConfig.maps_path) as f:
            one_hot_maps = json.load(f)
        with open(ExtractorConfig.fields_path) as f:
            schema_fields = json.load(f)
            
        retriever = SparseRetriever(
            schema_fields=schema_fields,
            one_hot_maps=one_hot_maps,
            crime_keywords_path=ExtractorConfig.KEYWORDS_FILE
        )
        retriever.load_precomputed_data(ExtractorConfig.DB_PATH)
        logger.debug("ç¨€ç–æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–ç¨€ç–æ£€ç´¢å™¨å¤±è´¥: {e}")
        raise e

    # 3. åˆå§‹åŒ–è¯­æ–™ç®¡ç†å™¨ (è¯»å–åŸå§‹æ•°æ®ç”¨äºå±•ç¤º)
    try:
        corpus_manager = CorpusManager(str(ExtractorConfig.INPUT_DATASET))
        corpus_manager.load_index()
        # æµ‹è¯•è¯»å–ç¬¬ä¸€æ¡æ•°æ®
        first_doc = corpus_manager.get_doc(0)
        if not first_doc:
            logger.error("æ— æ³•è¯»å–è¯­æ–™æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶!")
        else:
            logger.debug(f"æˆåŠŸåŠ è½½è¯­æ–™æ•°æ®ï¼Œç¤ºä¾‹æ•°æ®: {json.dumps(first_doc['meta'], ensure_ascii=False)}")
    except Exception as e:
        logger.error(f"è¯­æ–™ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

    # 4. åˆå§‹åŒ– Schema å’Œ Keywords ç®¡ç†å™¨ 
    try:
        schema_keywords_manager = SchemaKeywordsManager(ExtractorConfig.SCHEMA_PATH, ExtractorConfig.KEYWORDS_PATH)
        schema_keywords_manager.load_index()
    except Exception as e:
        logger.warning(f"Schemaå’ŒKeywordsç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

    logger.info("âœ… æœåŠ¡å¯åŠ¨å®Œæˆï¼Œå‡†å¤‡å°±ç»ªã€‚")
    try:
        yield 
    finally:
        if corpus_manager:
            corpus_manager.close()
        if schema_keywords_manager: 
            schema_keywords_manager.close()
        logger.info("æœåŠ¡å·²å…³é—­ã€‚")

app = FastAPI(title="Legal Sparse Retrieval Service", lifespan=lifespan)

# ---------------------------------------------------------
# 3. æ•°æ®æ¨¡å‹ (Pydantic)
# ---------------------------------------------------------
class SearchRequest(BaseModel):
    fact: str
    top_k: int = 5

class BatchSearchRequest(BaseModel):
    facts: List[str]
    top_k: int = 5

class SearchResult(BaseModel):
    fact_id: str
    score: float
    rank: int
    fact: str = ""
    accusation: List[str] = []
    relevant_articles: List[str] = []
    imprisonment: Dict[str, Any] = {}  
    punish_of_money: float = 0.0      
    criminals: List[str] = []        
    matched_keywords: Dict[str, int] = {}
    query_schema: Optional[Dict[str, Any]] = None
    document_schema: Optional[Dict[str, Any]] = None
    document_keywords: Optional[Dict[str, int]] = None
    laws: List[str] = []
    metadata: Optional[Dict[str, Any]] = None
    orig_score: Optional[float] = None



# ---------------------------------------------------------
# 4. æ¥å£å®ç°
# ---------------------------------------------------------

def _process_single_search(fact: str, top_k: int) -> List[SearchResult]:
    """å†…éƒ¨å¤„ç†å•æ¡æ£€ç´¢é€»è¾‘"""
    # 1. æå–å…³é”®è¯é¢‘æ¬¡
    extraction = extractor.extract(fact)
    query_counts = extraction.get("keyword_counts", {})
    
    # 2. æå–ç»“æ„åŒ– Schema 
    query_schema_flat = {}
    raw_query_schema = {} 

    if universal_extractor and retriever and getattr(retriever, 'schema_fields', []):
        # æå–åŸå§‹åµŒå¥—ç»“æ„
        raw_query_schema = universal_extractor.extract_from_fact(fact)
        
        # æ‰å¹³åŒ–å¤„ç†ç”¨äºæ£€ç´¢
        def flatten(x, name=''):
            out = {}
            if isinstance(x, dict):
                for a in x: 
                    out.update(flatten(x[a], name + a + '.'))
            else: 
                out[name[:-1]] = x
            return out
            
        query_schema_flat = flatten(raw_query_schema)

    # 3. æ‰§è¡Œæ£€ç´¢
    try:
        raw_results = retriever.search(query_schema_flat, query_counts, top_k=top_k)
    except ValueError as e:
        logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
        return []
    
    # 4. æ ¼å¼åŒ–ç»“æœ
    logger.debug(f"Query Schema: {raw_query_schema}")
    logger.debug(f"Query Keywords: {query_counts}")
    
    formatted_results = []
    for rank, res in enumerate(raw_results):
        try:
            doc_info = corpus_manager.get_doc(res['index']) if corpus_manager else {}
            if not doc_info:
                logger.error(f"æ— æ³•è·å–æ–‡æ¡£ä¿¡æ¯: index={res['index']}")
                continue
                
            meta = doc_info.get("meta", {})
            if not meta:
                logger.error(f"æ–‡æ¡£ç¼ºå°‘metaä¿¡æ¯: index={res['index']}")
                
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            logger.debug(f"å¤„ç†æ–‡æ¡£: index={res['index']}, meta={json.dumps(meta, ensure_ascii=False)}")
            

            document_schema = {}
            document_keywords = {}
            if schema_keywords_manager:
                doc_schema, doc_keywords_dict = schema_keywords_manager.get_schema_and_keywords(res['index'])
                document_schema = doc_schema
                document_keywords = doc_keywords_dict
            else:
                logger.warning(f"SchemaKeywordsManager æœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å– index {res['index']} çš„ schema å’Œ keywordsã€‚")
            # --- ---

            # è°ƒè¯•æ—¥å¿—
            logger.debug(f"Doc Info: {json.dumps(doc_info, ensure_ascii=False)}")
            logger.debug(f"Meta Info: {json.dumps(meta, ensure_ascii=False)}")
            
            formatted_results.append(SearchResult(
                fact_id=str(res['id']),
                score=float(res['score']),
                rank=rank + 1,
                fact=doc_info.get("fact", ""),
                accusation=meta.get("accusation", []),
                relevant_articles=meta.get("relevant_articles", []),
                imprisonment=meta.get("term_of_imprisonment", {
                    "death_penalty": False,
                    "life_imprisonment": False,
                    "imprisonment": 0
                }),
                punish_of_money=float(meta.get("punish_of_money", 0)),
                criminals=meta.get("criminals", []),
                matched_keywords=query_counts,
                query_schema=raw_query_schema,
                document_schema=document_schema,     
                document_keywords=document_keywords,
                laws=meta.get("relevant_articles", []),
                metadata={
                    "raw_meta_sample": meta,
                    "doc_index": res.get("index"),
                    "doc_id": res.get("id")
                }
            ))
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ (FACT_ID: {res.get('id')}): {e}", exc_info=True)
            continue
    
    return formatted_results

@app.post("/search", response_model=List[SearchResult])
async def search(request: SearchRequest):
    if not retriever or not extractor:
        raise HTTPException(status_code=503, detail="Service initializing")
    
    try:
        start_time = time.time()
        results = _process_single_search(request.fact, request.top_k)
        elapsed = (time.time() - start_time) * 1000
        
        logger.info(f"Search processed in {elapsed:.2f}ms. Found {len(results)} results.")
        
        for r in results:
            try:
                fact_text = (r.fact or "").replace("\n", " ").strip()
                logger.info(
                    f"\n{'='*80}\n"
                    f"  Search Result [Rank {r.rank}]:\n"
                    f"  Score: {r.score:.4f}\n"
                    f"  Fact_ID: {r.fact_id}\n"
                    f"  ç½ªå: {r.accusation}\n"
                    f"  æ³•æ¡: {r.relevant_articles}\n"
                    f"  åˆ‘æœŸ: {r.imprisonment}\n"
                    f"  ç½šé‡‘: {r.punish_of_money}å…ƒ\n"
                    f"  çŠ¯ç½ªäºº: {r.criminals}\n"
                    f"  Schema: {json.dumps(r.query_schema, ensure_ascii=False, indent=2)}\n"
                    f"  Keywords: {json.dumps(r.matched_keywords, ensure_ascii=False, indent=2)}\n"
                    f"  æ¡ˆä»¶: {fact_text[:200]}...\n"
                    f"{'='*80}"
                )
            except Exception as e:
                logger.warning(f"æ‰“å°ç»“æœè¯¦æƒ…å¤±è´¥: {e}")
        
        return results
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))



@app.post("/batch_search", response_model=List[List[SearchResult]])
async def batch_search(request: BatchSearchRequest):
    if not retriever or not extractor:
        raise HTTPException(status_code=503, detail="Service initializing")
    
    try:
        start_time = time.time()
        
        batch_results = [None] * len(request.facts)
        
        max_workers = min(16, len(request.facts))
        
        logger.info(f"Starting batch search for {len(request.facts)} items with {max_workers} threads...")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_idx = {
                executor.submit(_process_single_search, fact, request.top_k): i
                for i, fact in enumerate(request.facts)
            }
            
            # è·å–ç»“æœ
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    batch_results[idx] = future.result()
                except Exception as e:
                    logger.error(f"Error processing item {idx}: {e}")
                    batch_results[idx] = [] 

        elapsed = (time.time() - start_time) * 1000
        logger.info(f"Batch search ({len(request.facts)} items) processed in {elapsed:.2f}ms. Avg: {elapsed/len(request.facts):.2f}ms/item")
        return batch_results
        
    except Exception as e:
        logger.error(f"Batch Search Error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=GlobalConfig.PORT,
        log_level="info", 
        reload=False
    )
