import json
import os
from tqdm import tqdm
from scipy import sparse
from config import ExtractorConfig

def flatten_json(y):
    out = {}
    def flatten(x, name=''):
        if isinstance(x, dict):
            for a in x:
                flatten(x[a], name + a + '.')
        else:
            out[name[:-1]] = x
    flatten(y)
    return out

def load_keywords_map():
    KEYWORDS_PATH = ExtractorConfig.KEYWORDS_FILE
    print(f"ğŸ“– åŠ è½½ç½ªåå…³é”®è¯å®šä¹‰: {KEYWORDS_PATH}")
    with open(KEYWORDS_PATH, 'r', encoding='utf-8') as f:
        kw_data = json.load(f)
    # æå–æ‰€æœ‰å…³é”®è¯ï¼ˆæ‰å¹³åŒ–ï¼‰
    all_keywords = sorted(list(set([kw for keyword_list in kw_data.values() for kw in keyword_list])))
    kw_map = {kw: i for i, kw in enumerate(all_keywords)}
    return kw_map, all_keywords

def build_sparse_matrix():
    SCHEMA_DATA_PATH = ExtractorConfig.SCHEMA_PATH
    KEYWORD_DATA_PATH = ExtractorConfig.KEYWORDS_PATH
    OUTPUT_DB_PATH = ExtractorConfig.DB_PATH
    SCHEMA_FIELDS_PATH = os.path.join(ExtractorConfig.OUTPUT_DIR, "schema_fields.json")
    ONE_HOT_MAPS_PATH = os.path.join(ExtractorConfig.OUTPUT_DIR, "one_hot_maps.json")

    if not os.path.exists(SCHEMA_DATA_PATH):
        print(f"âŒ æ‰¾ä¸åˆ° Schema æ–‡ä»¶: {SCHEMA_DATA_PATH}")
        return
    if not os.path.exists(KEYWORD_DATA_PATH):
        print(f"âŒ æ‰¾ä¸åˆ° Keyword æ–‡ä»¶: {KEYWORD_DATA_PATH}")
        return

    print("1. åŠ è½½ Schema é…ç½®...")
    if not os.path.exists(SCHEMA_FIELDS_PATH) or not os.path.exists(ONE_HOT_MAPS_PATH):
        print("âŒ ç¼ºå°‘ schema_fields.json æˆ– one_hot_maps.jsonï¼Œè¯·å…ˆè¿è¡Œ build_schema_index_from_config.py")
        return

    with open(SCHEMA_FIELDS_PATH, 'r', encoding='utf-8') as f:
        schema_fields = json.load(f)
    with open(ONE_HOT_MAPS_PATH, 'r', encoding='utf-8') as f:
        one_hot_maps = json.load(f)

    kw_map, all_keywords = load_keywords_map()

    schema_offsets = {}
    current_offset = 0
    for field in schema_fields:
        schema_offsets[field] = current_offset
        current_offset += len(one_hot_maps[field])
    schema_dim = current_offset
    keyword_dim = len(all_keywords)
    total_dim = schema_dim + keyword_dim

    print(f"ğŸ“Š çŸ©é˜µç»´åº¦ç»Ÿè®¡:")
    print(f" - Schema ç»´åº¦: {schema_dim}")
    print(f" - Keyword ç»´åº¦: {keyword_dim}")
    print(f" - æ€»ç»´åº¦: {total_dim}")

    rows = []
    cols = []
    data = []
    row_idx = 0

    print(f"2. æ­£åœ¨å¹¶è¡Œæ‰«æä¸¤ä¸ªæ–‡ä»¶...")
    print(f" Schema: {SCHEMA_DATA_PATH}")
    print(f" Keyword: {KEYWORD_DATA_PATH}")

    with open(SCHEMA_DATA_PATH, 'r', encoding='utf-8') as f_schema, \
         open(KEYWORD_DATA_PATH, 'r', encoding='utf-8') as f_keyword:
        for line_s, line_k in tqdm(zip(f_schema, f_keyword), desc="Building Matrix"):
            line_s = line_s.strip()
            line_k = line_k.strip()
            if not line_s or not line_k:
                continue
            try:
                item_s = json.loads(line_s)
                item_k = json.loads(line_k)

                u_fact = item_s.get("universal_fact")
                if u_fact:
                    flat_fact = flatten_json(u_fact)
                    for field, value in flat_fact.items():
                        if field in one_hot_maps and str(value) in one_hot_maps[field]:
                            val_idx = one_hot_maps[field][str(value)]
                            col_idx = schema_offsets[field] + val_idx
                            rows.append(row_idx)
                            cols.append(col_idx)
                            data.append(1.0)

                kw_counts = {}
                if "sparse_extraction" in item_k and "keyword_counts" in item_k["sparse_extraction"]:
                    kw_counts = item_k["sparse_extraction"]["keyword_counts"]
                elif "keyword_counts" in item_k:
                    kw_counts = item_k["keyword_counts"]

                if kw_counts:
                    for kw, count in kw_counts.items():
                        if kw in kw_map:
                            col_idx = schema_dim + kw_map[kw]
                            rows.append(row_idx)
                            cols.append(col_idx)
                            data.append(float(count))
                row_idx += 1
            except Exception as e:
                pass

    print(f"âœ… æ‰«æå®Œæˆã€‚å…± {row_idx} æ¡æ•°æ®ã€‚")

    print("3. è½¬æ¢å¹¶ä¿å­˜çŸ©é˜µ...")
    matrix = sparse.csr_matrix((data, (rows, cols)), shape=(row_idx, total_dim))
    sparse.save_npz(OUTPUT_DB_PATH, matrix)
    print(f"ğŸ’¾ çŸ©é˜µå·²ä¿å­˜è‡³: {OUTPUT_DB_PATH}")
    print(f" æ–‡ä»¶å¤§å°: {os.path.getsize(OUTPUT_DB_PATH) / 1024 / 1024:.2f} MB")

if __name__ == "__main__":
    build_sparse_matrix()