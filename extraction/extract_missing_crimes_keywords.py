# extract_missing_crimes_keywords.py
import json
import re
from collections import defaultdict
import os
import argparse

try:
    import hanlp
    HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
    USE_HANLP = True
except Exception as e:
    print(f"âŒ æ— æ³•åŠ è½½ HanLP: {e}")
    exit(1)


def load_stopwords():
    stopwords = set()
    if os.path.exists("stopwords.txt"):
        with open("stopwords.txt", "r", encoding="utf-8") as f:
            stopwords.update(line.strip() for line in f if line.strip())
    else:
        basic = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸ª",
                 "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½"}
        judicial = {"ç»æŸ¥", "æœ¬é™¢è®¤ä¸º", "ç»¼ä¸Š", "ä¾æ³•", "åˆ¤å¤„", "å®¡ç†", "æŸ¥æ˜", "ä¸Šè¿°", "ä¾ç…§",
                    "è§„å®š", "åº”å½“", "ä¾æ³•äºˆä»¥", "æèµ·å…¬è¯‰", "å‘æœ¬é™¢", "è¯·æ±‚ä¾æ³•", "è¢«å‘Šäºº",
                    "è¢«å®³äºº", "ä¾›è¿°", "è¯è¨€", "é‰´å®šæ„è§", "è¯æ®", "äº‹å®æ¸…æ¥š", "è¯æ®ç¡®å®",
                    "å……åˆ†", "æ„æˆ", "æ£€å¯Ÿé™¢", "å…¬å®‰æœºå…³", "æŠ•æ¡ˆ", "è‡ªé¦–", "è°…è§£", "èµ”å¿"}
        stopwords = basic | judicial
    return stopwords


def tokenize_with_hanlp(text, stopwords):
    if not isinstance(text, str) or not text.strip():
        return []
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç‚¹ã€æ–œæ ï¼ˆå¦‚ IPã€ç¼–å·ï¼‰
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9./]", " ", text)

    try:
        doc = HanLP(text, tasks=['tok/fine', 'ner/msra'])
        words = doc['tok/fine']
        ner_entities = doc['ner/msra']
        entity_set = set(ent[0].lower() for ent in ner_entities)

        result = []
        for w in words:
            w = w.strip().lower()
            if len(w) < 2:
                continue
            if w in stopwords:
                continue
            if w in entity_set:
                continue
            if re.match(r'^[a-z]*\d+[a-z]*$', w) or w.isdigit():
                continue
            # è¿‡æ»¤æ³›åŒ–æ³•å¾‹æœ¯è¯­
            if w in {"è´¢ç‰©", "è¡Œä¸º", "å·¥å…·", "ç‰©å“", "äººå‘˜", "äº‹æƒ…", "æ–¹å¼", "æ‰‹æ®µ", "è¿›è¡Œ", "å®æ–½",
                     "è¿‡ç¨‹", "æƒ…å†µ", "ç»“æœ", "ç›®çš„", "åœ°ç‚¹", "æ—¶é—´", "å†…å®¹", "éƒ¨åˆ†", "æ–¹é¢"}:
                continue
            result.append(w)
        return result
    except Exception as e:
        # å›é€€åˆ° jieba
        try:
            import jieba
            words = jieba.lcut(text)
            return [w.strip().lower() for w in words
                    if len(w.strip()) >= 2 and w.strip().lower() not in stopwords]
        except:
            return []


# ==============================
# ä¸»å‡½æ•°ï¼šä»…æå–æŒ‡å®šç½ªå
# ==============================
def main(jsonl_path, target_crimes, top_k=30, min_samples=1):
    print("ğŸ” åŠ è½½åœç”¨è¯...")
    stopwords = load_stopwords()

    target_set = set(target_crimes)
    print(f"ğŸ¯ ä»…æå–ä»¥ä¸‹ {len(target_set)} ä¸ªç½ªåçš„å…³é”®è¯:")
    for c in sorted(target_set):
        print(f"  - {c}")

    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šç”¨ dfï¼ˆæ–‡æ¡£é¢‘æ¬¡ï¼‰ä»£æ›¿ idx é›†åˆï¼Œé¿å… OOM
    crime_word_df = defaultdict(lambda: defaultdict(int))  # crime -> word -> doc_freq
    crime_doc_count = defaultdict(int)  # crime -> total_docs
    total_docs = 0  # æ€»â€œç½ªå-æ–‡æ¡£â€å¯¹æ•°ï¼ˆç”¨äº global normalizationï¼‰

    print(f"ğŸ“¥ æµå¼è¯»å–æ•°æ®: {jsonl_path}")
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for idx, line in enumerate(f):
            if not line.strip():
                continue
            try:
                data = json.loads(line)
                fact = data.get("fact", "")
                accusation = data.get("meta", {}).get("accusation", [])
                if not fact or not isinstance(accusation, list) or len(accusation) == 0:
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰ç›®æ ‡ç½ªåï¼ˆæ”¯æŒå¤šç½ªåï¼‰
                matched_crimes = [c for c in accusation if c in target_set]
                if not matched_crimes:
                    continue

                words = tokenize_with_hanlp(fact, stopwords)
                if not words:
                    continue

                # æ ·æœ¬å†…å»é‡ï¼ˆä¸€ä¸ªè¯åœ¨ä¸€ä¸ªæ ·æœ¬ä¸­åªè®¡ä¸€æ¬¡ï¼‰
                word_set = set(words)

                # ä¸ºæ¯ä¸ªåŒ¹é…ç½ªåæ›´æ–°ç»Ÿè®¡
                for crime in matched_crimes:
                    for w in word_set:
                        crime_word_df[crime][w] += 1
                    crime_doc_count[crime] += 1
                    total_docs += 1

                if total_docs % 5000 == 0:
                    print(f"  âœ… å·²å¤„ç† {total_docs} æ¡ç›®æ ‡æ¡ˆä»¶...")

            except Exception as e:
                # å¦‚éœ€è°ƒè¯•ï¼Œå¯å–æ¶ˆæ³¨é‡Šï¼š
                # print(f"âš ï¸ è·³è¿‡ç¬¬ {idx} è¡Œ: {e}")
                continue

    print(f"âœ… å…±å¤„ç† {total_docs} æ¡ç›®æ ‡æ¡ˆä»¶ï¼Œæ¶‰åŠ {len(crime_doc_count)} ä¸ªç½ªå")

    # ç”Ÿæˆå…³é”®è¯
    crime_keywords = {}
    global_total = total_docs  # æ‰€æœ‰ç›®æ ‡ç½ªåçš„æ€»æ–‡æ¡£æ•°ï¼ˆè¿‘ä¼¼ï¼‰

    for crime in target_set:
        if crime_doc_count[crime] < min_samples:
            print(f"âš ï¸  ç½ªå '{crime}' æ ·æœ¬ä¸è¶³ ({crime_doc_count[crime]} < {min_samples})ï¼Œä½¿ç”¨ç½ªåæœ¬èº«ä½œä¸ºå…³é”®è¯")
            crime_keywords[crime] = [crime]
            continue

        doc_count = crime_doc_count[crime]
        scores = {}
        for word, df_crime in crime_word_df[crime].items():
            if df_crime < 1:
                continue
            # è®¡ç®—è¯¥è¯åœ¨æ‰€æœ‰ç›®æ ‡ç½ªåä¸­çš„æ€»å‡ºç°æ–‡æ¡£æ•°
            df_global = sum(crime_word_df[c].get(word, 0) for c in target_set)
            # é˜²æ­¢é™¤é›¶
            score = (df_crime / doc_count) / (df_global / global_total + 1e-9)
            scores[word] = score

        # å– top-k
        top_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        keywords = [w for w, _ in top_words] if top_words else [crime]
        crime_keywords[crime] = keywords[:top_k]

    # ğŸ’¾ ä¿å­˜ç»“æœ
    output_file = "missing_crime_keywords.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(crime_keywords, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œæˆï¼å…³é”®è¯å·²ä¿å­˜è‡³: {output_file}")
    print("\né¢„è§ˆ:")
    for crime, kws in crime_keywords.items():
        print(f"{crime}: {kws[:5]}{'...' if len(kws) > 5 else ''}")


# ==============================
# CLI å…¥å£
# ==============================
if __name__ == "__main__":
    MISSING_CRIMES = [
        "ä¼ æ’­æ·«ç§½ç‰©å“",
        "åŒ…åº‡æ¯’å“çŠ¯ç½ªåˆ†å­",
        "ååŠ©ç»„ç»‡å–æ·«",
        "å·¨é¢è´¢äº§æ¥æºä¸æ˜",
        "å¼•è¯±ã€å®¹ç•™ã€ä»‹ç»å–æ·«",
        "å¼ºè¿«å–æ·«",
        "å¾‡ç§èˆå¼Šä¸å¾ã€å°‘å¾ç¨æ¬¾",
        "ç›—çªƒã€æŠ¢å¤ºæªæ”¯ã€å¼¹è¯ã€çˆ†ç‚¸ç‰©ã€å±é™©ç‰©è´¨",
        "ç»„ç»‡å–æ·«",
        "ç»æµçŠ¯",
        "éæ³•ä¹°å–ã€è¿è¾“ã€æºå¸¦ã€æŒæœ‰æ¯’å“åŸæ¤ç‰©ç§å­ã€å¹¼è‹—",
        "éæ³•æ”¶è´­ã€è¿è¾“ã€å‡ºå”®çè´µã€æ¿’å±é‡ç”ŸåŠ¨ç‰©ã€çè´µã€æ¿’å±é‡ç”ŸåŠ¨ç‰©åˆ¶å“"
    ]

    parser = argparse.ArgumentParser(description="é«˜æ•ˆæå–æŒ‡å®šç½ªåçš„å…³é”®è¯ï¼ˆä»…å¤„ç†ç›®æ ‡ç±»ï¼‰")
    parser.add_argument("--jsonl_file", default=r"F:\LegalAgent\dataset\final_all_data\first_stage\train.json", help="æ¡ˆä»¶JSONLè·¯å¾„")
    parser.add_argument("--top_k", type=int, default=25, help="æ¯ç½ªåå…³é”®è¯æ•°é‡")
    parser.add_argument("--min_samples", type=int, default=1, help="æœ€å°æ ·æœ¬æ•°ï¼ˆè®¾ä¸º1ç¡®ä¿å…¨è¦†ç›–ï¼‰")

    args = parser.parse_args()

    main(
        jsonl_path=args.jsonl_file,
        target_crimes=MISSING_CRIMES,
        top_k=args.top_k,
        min_samples=args.min_samples
    )