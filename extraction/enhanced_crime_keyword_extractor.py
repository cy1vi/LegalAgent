# final_crime_keyword_extractor_hanlp.py
import json
import re
from collections import defaultdict
import os
import argparse
import tempfile

# ==============================
# å°è¯•å¯¼å…¥ HanLPï¼ˆå¿…é¡»å®‰è£… hanlpï¼‰
# ==============================
try:
    import hanlp
    # åŠ è½½å¤šä»»åŠ¡æ¨¡å‹ï¼ˆåŒ…å«åˆ†è¯ã€è¯æ€§ã€NERï¼‰
    HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)
    USE_HANLP = True
    print("âœ… HanLP å¤šä»»åŠ¡æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆå« NERï¼‰")
except Exception as e:
    print(f"âŒ æ— æ³•åŠ è½½ HanLP: {e}")
    print("è¯·è¿è¡Œ: pip install hanlp -i https://pypi.tuna.tsinghua.edu.cn/simple")
    exit(1)

# ==============================
# 1. åœç”¨è¯åŠ è½½
# ==============================
def load_stopwords():
    stopwords = set()
    if os.path.exists("stopwords.txt"):
        with open("stopwords.txt", "r", encoding="utf-8") as f:
            stopwords.update(line.strip() for line in f if line.strip())
    else:
        basic = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸ª",
                 "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½"}
        judicial = {"ç»æŸ¥", "æœ¬é™¢è®¤ä¸º", "ç»¼ä¸Š", "ä¾æ³•", "åˆ¤å¤„", "å®¡ç†", "æŸ¥æ˜", "ä¸Šè¿°", "ä¾ç…§",
                    "è§„å®š", "åº”å½“", "ä¾æ³•äºˆä»¥", "æèµ·å…¬è¯‰", "å‘æœ¬é™¢", "è¯·æ±‚ä¾æ³•", "è¢«å‘Šäºº",
                    "è¢«å®³äºº", "ä¾›è¿°", "è¯è¨€", "é‰´å®šæ„è§", "è¯æ®", "äº‹å®æ¸…æ¥š", "è¯æ®ç¡®å®",
                    "å……åˆ†", "æ„æˆ", "æ£€å¯Ÿé™¢", "å…¬å®‰æœºå…³", "æŠ•æ¡ˆ", "è‡ªé¦–", "è°…è§£", "èµ”å¿"}
        stopwords = basic | judicial
    return stopwords

# ==============================
# 2. æ„å»ºæ³•å¾‹è¯å…¸ï¼ˆç”¨äºå¢å¼ºå…³é”®è¯ï¼‰
# ==============================
def build_legal_dict(crimes_txt_path=None):
    legal_words = set()
    if crimes_txt_path and os.path.exists(crimes_txt_path):
        with open(crimes_txt_path, "r", encoding="utf-8") as f:
            for line in f:
                crime = line.strip()
                if crime:
                    parts = re.split(r'[ã€ï¼Œ\s]+', crime)
                    for part in parts:
                        if len(part) >= 2:
                            legal_words.add(part)
    else:
        builtin = [
            "æ•…æ„ä¼¤å®³", "ç›—çªƒ", "æŠ¢åŠ«", "è¯ˆéª—", "å±é™©é©¾é©¶", "äº¤é€šè‚‡äº‹",
            "å®¹ç•™ä»–äººå¸æ¯’", "è´©å–æ¯’å“", "å¼ºå¥¸", "éæ³•æŒæœ‰æªæ”¯", "æ”¾ç«",
            "çˆ†ç‚¸", "ç»‘æ¶", "æ‹å–å¦‡å¥³å„¿ç«¥", "è´ªæ±¡", "å—è´¿", "èŒåŠ¡ä¾µå ",
            "éæ³•ç»è¥", "å¼€è®¾èµŒåœº", "åˆåŒè¯ˆéª—", "ä¿¡ç”¨å¡è¯ˆéª—", "æ´—é’±",
            "å‡å†’æ³¨å†Œå•†æ ‡", "ç”Ÿäº§é”€å”®å‡è¯", "æ±¡æŸ“ç¯å¢ƒ", "å¦¨å®³å…¬åŠ¡",
            "èšä¼—æ–—æ®´", "ä»¥å±é™©æ–¹æ³•å±å®³å…¬å…±å®‰å…¨", "è™šå¼€å‘ç¥¨", "é€ƒç¨",
            "å¯»è¡…æ»‹äº‹", "å¸®åŠ©æ¯ç­ä¼ªé€ è¯æ®"
        ]
        for crime in builtin:
            parts = re.split(r'[ã€ï¼Œ\s]+', crime)
            for part in parts:
                if len(part) >= 2:
                    legal_words.add(part)

    terms = [
        "éšæ„æ®´æ‰“", "è¿½é€æ‹¦æˆª", "è¾±éª‚æå“", "èµ·å“„é—¹äº‹", "ä»»æ„æŸæ¯", "å ç”¨è´¢ç‰©",
        "ä¼ªé€ è¯æ®", "æ¯ç­è¯æ®", "å¸®åŠ©æ¯ç­", "éšåŒ¿è¯æ®", "ä½œå‡è¯æ˜",
        "é†‰é…’é©¾é©¶", "è¡€æ¶²é…’ç²¾å«é‡", "æŒæ¢°", "è½»ä¼¤", "é‡ä¼¤", "æ­»äº¡", "é€ƒé€¸",
        "ç§˜å¯†çªƒå–", "å†’å……", "è™šæ„äº‹å®", "éæ³•å æœ‰", "è®¡ç®—æœºç³»ç»Ÿ"
    ]
    legal_words.update(terms)

    dict_path = os.path.join(tempfile.gettempdir(), "legal_dict_temp.txt")
    with open(dict_path, "w", encoding="utf-8") as f:
        for word in sorted(legal_words):
            if len(word) >= 2:
                f.write(f"{word}\n")
    return dict_path

# ==============================
# 3. ä½¿ç”¨ HanLP è¿›è¡Œåˆ†è¯ + NER è¿‡æ»¤
# ==============================
def tokenize_with_hanlp(text, stopwords):
    if not isinstance(text, str) or not text.strip():
        return []
    # ä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—ã€./ï¼ˆå¦‚ mg/100mlï¼‰
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9./]", " ", text)
    
    try:
        doc = HanLP(text, tasks=['tok/fine', 'ner/msra'])
        words = doc['tok/fine']
        ner_entities = doc['ner/msra']
        
        # æå–æ‰€æœ‰è¢«æ ‡è®°ä¸ºå®ä½“çš„è¯ï¼ˆè½¬å°å†™ï¼‰
        entity_set = set(ent[0].lower() for ent in ner_entities)
        
        result = []
        for w in words:
            w = w.strip().lower()
            if len(w) < 2:
                continue
            if w in stopwords:
                continue
            if w in entity_set:  # è¿‡æ»¤äººåã€åœ°åã€ç»„ç»‡åç­‰
                continue
            if re.match(r'^[a-z]*\d+[a-z]*$', w) or w.isdigit():
                continue
            if w in {"è´¢ç‰©", "è¡Œä¸º", "å·¥å…·", "ç‰©å“", "äººå‘˜", "äº‹æƒ…", "æ–¹å¼", "æ‰‹æ®µ", "è¿›è¡Œ", "å®æ–½"}:
                continue
            result.append(w)
        return result
    except Exception as e:
        # å›é€€ï¼šç®€å•åˆ†è¯ï¼ˆå®é™…å¾ˆå°‘è§¦å‘ï¼‰
        import jieba
        words = jieba.lcut(text)
        return [w.strip().lower() for w in words if len(w.strip()) >= 2 and w.strip() not in stopwords]

# ==============================
# 4. æ³•å¾‹è¡Œä¸ºè¯ç™½åå•ï¼ˆç”¨äºå›é€€å’Œæ ¡éªŒï¼‰
# ==============================
LEGAL_BEHAVIOR_KEYWORDS = {
    "å¯»è¡…æ»‹äº‹": ["æ®´æ‰“", "è¾±éª‚", "æå“", "è¿½é€", "æ‹¦æˆª", "èµ·å“„", "é—¹äº‹", "æŸæ¯", "å ç”¨", "éšæ„"],
    "å¸®åŠ©æ¯ç­ã€ä¼ªé€ è¯æ®": ["æ¯ç­", "ä¼ªé€ ", "è¯æ®", "éšåŒ¿", "æŠ›å¼ƒ", "æ©åŸ‹", "ä½œå‡", "é€ å‡"],
    "æ•…æ„ä¼¤å®³": ["æ®´æ‰“", "æ‰“ä¼¤", "æ…", "ç ", "å‡»æ‰“", "ä¼¤å®³", "é‡ä¼¤", "è½»ä¼¤"],
    "ç›—çªƒ": ["çªƒå–", "å·", "ç§˜å¯†", "æ‰’çªƒ", "å…¥æˆ·", "ç›—å–"],
    "è¯ˆéª—": ["éª—å–", "è™šæ„", "éšç’", "è°ç§°", "å†’å……", "è¿”åˆ©", "æŠ•èµ„", "å¹³å°"],
    "å±é™©é©¾é©¶": ["é†‰é…’", "é…’ç²¾", "é©¾é©¶", "æœºåŠ¨è½¦", "è¡€æ¶²", "è¶…æ ‡"],
    "è´©å–æ¯’å“": ["è´©å–", "æ¯’å“", "å†°æ¯’", "æµ·æ´›å› ", "å¤§éº»", "äº¤æ˜“"],
    "å®¹ç•™ä»–äººå¸æ¯’": ["å®¹ç•™", "å¸æ¯’", "æä¾›åœºæ‰€", "å¸é£Ÿ"],
    "äº¤é€šè‚‡äº‹": ["è‚‡äº‹", "é€ƒé€¸", "æ’", "è‡´äººæ­»äº¡", "è‡´äººé‡ä¼¤", "è¿åäº¤è§„"],
    "æŠ¢åŠ«": ["æŠ¢åŠ«", "æš´åŠ›", "èƒè¿«", "æŠ¢èµ°", "æŒæ¢°"],
}

KEYWORD_FIELD_RULES = {
    "mentions_violence": ["æ®´æ‰“", "æ‰“ä¼¤", "æ‹³è„š", "æš´åŠ›", "ç ¸", "è¸¢", "æ…", "æŒæ¢°"],
    "mentions_impersonation": ["å†’å……", "å‡å†’", "ä¼ªè£…", "è°ç§°"],
    "mentions_alcohol": ["é†‰é…’", "é…’ç²¾", "é¥®é…’", "è¡€æ¶²é…’ç²¾", "é…’å"],
    "mentions_vehicle": ["æœºåŠ¨è½¦", "æ±½è½¦", "é©¾é©¶", "é“è·¯", "è½¦è¾†", "äº¤é€š", "è¡Œé©¶"],
    "mentions_drugs": ["æ¯’å“", "å¸æ¯’", "å†°æ¯’", "æµ·æ´›å› ", "å¤§éº»", "å®¹ç•™", "è´©æ¯’"],
    "mentions_financial_fraud": ["è¯ˆéª—", "è¿”åˆ©", "æŠ•èµ„", "å¹³å°", "è½¬è´¦", "ç†è´¢", "é›†èµ„"],
    "mentions_secret_theft": ["ç§˜å¯†", "è¶äººä¸å¤‡", "å…¥æˆ·", "çªƒå–", "å·", "æ‰’çªƒ"],
    "mentions_public_disorder": ["å…¬å…±åœºæ‰€", "èµ·å“„", "æ‰°ä¹±", "èšä¼—", "é—¹äº‹", "æ»‹äº‹"],
    "mentions_firearms": ["æªæ”¯", "å¼¹è¯", "çˆ†ç‚¸ç‰©", "ç«è¯", "æŒæª"],
    "mentions_computer": ["è®¡ç®—æœº", "ç³»ç»Ÿ", "ç¨‹åº", "é»‘å®¢", "ä¾µå…¥", "æ§åˆ¶"]
}

# ==============================
# 5. ä¸»æµç¨‹
# ==============================
def main(jsonl_path, crimes_txt_path=None, top_k=30, min_samples=5):
    print("ğŸ” åŠ è½½åœç”¨è¯...")
    stopwords = load_stopwords()

    print("ğŸ“š æ„å»ºæ³•å¾‹è¯å…¸...")
    dict_path = build_legal_dict(crimes_txt_path)

    print(f"ğŸ“¥ æµå¼è¯»å–æ•°æ®: {jsonl_path}")

    global_df = defaultdict(int)
    crime_word_docs = defaultdict(lambda: defaultdict(set))
    crime_doc_count = defaultdict(int)
    total_docs = 0

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for idx, line in enumerate(f):
            if not line.strip():
                continue
            try:
                data = json.loads(line)
                fact = data.get("fact", "")
                accusation = data.get("meta", {}).get("accusation", [])
                if fact and accusation:
                    crime = accusation[0]
                    words = tokenize_with_hanlp(fact, stopwords)
                    if not words:
                        continue

                    word_set = set(words)
                    for w in word_set:
                        crime_word_docs[crime][w].add(idx)
                        global_df[w] += 1
                    crime_doc_count[crime] += 1
                    total_docs += 1

                    if total_docs % 20000 == 0:
                        print(f"  âœ… å·²å¤„ç† {total_docs} æ¡æ¡ˆä»¶...")

            except json.JSONDecodeError:
                continue

    print(f"âœ… å…±å¤„ç† {total_docs} æ¡æœ‰æ•ˆæ¡ˆä»¶ï¼Œæ¶‰åŠ {len(crime_doc_count)} ç§ç½ªå")

    valid_crimes = {crime for crime, cnt in crime_doc_count.items() if cnt >= min_samples}
    crime_keywords = {}

    print("ğŸ“Š è®¡ç®—åˆ¤åˆ«æ€§å…³é”®è¯ï¼ˆåŸºäºæ–‡æ¡£é¢‘ç‡ï¼‰...")
    for crime in valid_crimes:
        doc_count = crime_doc_count[crime]
        scores = {}
        for word, doc_ids in crime_word_docs[crime].items():
            df_crime = len(doc_ids)
            df_global = global_df[word]
            if df_crime < 2:
                continue
            score = (df_crime / doc_count) / (df_global / total_docs + 1e-9)
            scores[word] = score

        top_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        keywords = [w for w, _ in top_words]
        crime_keywords[crime] = keywords

    # ==============================
    # ğŸ” å›é€€ + ç™½åå•å¢å¼º
    # ==============================
    print("ğŸ”„ åº”ç”¨ç™½åå•å¢å¼ºä¸å›é€€ç­–ç•¥...")
    for crime in list(valid_crimes):
        keywords = crime_keywords.get(crime, [])
        whitelist = LEGAL_BEHAVIOR_KEYWORDS.get(crime, [])
        has_whitelist = any(kw in keywords for kw in whitelist)

        if not has_whitelist or len(keywords) < 3:
            enhanced = whitelist + [crime]
            existing_good = [w for w in keywords if len(w) >= 2 and not w.isdigit()]
            all_candidates = enhanced + existing_good
            seen = set()
            dedup = []
            for w in all_candidates:
                if w not in seen:
                    dedup.append(w)
                    seen.add(w)
            crime_keywords[crime] = dedup[:top_k]

    # ==============================
    # ğŸ’¾ ä¿å­˜ç»“æœ
    # ==============================
    print("ğŸ’¾ ä¿å­˜å…³é”®è¯æ–‡æœ¬...")
    with open("crime_keywords.txt", "w", encoding="utf-8") as f:
        for crime in sorted(crime_keywords.keys()):
            keywords = crime_keywords[crime]
            f.write(f"\n=== {crime} ===\n")
            for word in keywords:
                f.write(f"{word}\n")

    print("ğŸ§© ç”Ÿæˆå­—æ®µæ˜ å°„...")
    mapping_output = {}
    for crime, keywords in crime_keywords.items():
        suggested = {}
        for field, triggers in KEYWORD_FIELD_RULES.items():
            matched = [w for w in keywords if w in triggers]
            if matched:
                suggested[field] = matched
        mapping_output[crime] = {
            "raw_keywords": keywords,
            "suggested_fields": suggested
        }

    with open("keyword_to_field_mapping.json", "w", encoding="utf-8") as f:
        json.dump(mapping_output, f, ensure_ascii=False, indent=2)

    print("âœ… å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š")
    print("   - crime_keywords.txt")
    print("   - keyword_to_field_mapping.json")

# ==============================
# 6. CLI å…¥å£
# ==============================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æœ€ç»ˆç‰ˆç½ªåå…³é”®è¯æå–å™¨ï¼ˆHanLP ç‰ˆï¼Œå« NER è¿‡æ»¤ï¼‰")
    parser.add_argument("--jsonl_file", default=r"F:\LegalAgent\dataset\final_all_data\first_stage\test.json", help="æ¡ˆä»¶JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--crimes_txt", default=r"F:\LegalAgent\dataset\final_all_data\meta\accu.txt", help="ç½ªååˆ—è¡¨txt")
    parser.add_argument("--top_k", type=int, default=30, help="æ¯ç½ªåå…³é”®è¯æ•°")
    parser.add_argument("--min_samples", type=int, default=5, help="ç½ªåæœ€å°æ ·æœ¬æ•°")

    args = parser.parse_args()
    main(
        jsonl_path=args.jsonl_file,
        crimes_txt_path=args.crimes_txt,
        top_k=args.top_k,
        min_samples=args.min_samples
    )